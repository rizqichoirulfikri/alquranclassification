{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Lib"
      ],
      "metadata": {
        "id": "yKSFqGai5fXR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEbBm_81GP-A",
        "outputId": "f7e7bcbc-28c3-44b5-e342-5c6a9d1f9fbb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "!pip install neupy\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import hamming_loss,classification_report,f1_score,zero_one_loss\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import VotingRegressor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting neupy\n",
            "  Downloading neupy-0.8.2-py2.py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from neupy) (1.4.1)\n",
            "Collecting tensorflow<1.14.0,>=1.10.1\n",
            "  Downloading tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from neupy) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from neupy) (1.19.5)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from neupy) (3.2.2)\n",
            "Collecting graphviz==0.5.1\n",
            "  Downloading graphviz-0.5.1-py2.py3-none-any.whl (14 kB)\n",
            "Collecting progressbar2==3.34.3\n",
            "  Downloading progressbar2-3.34.3-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: python-utils>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2==3.34.3->neupy) (3.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->neupy) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->neupy) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->neupy) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->neupy) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.1->neupy) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (1.1.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (0.4.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 71.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (0.37.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (1.1.2)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 50.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (1.43.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<1.14.0,>=1.10.1->neupy) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<1.14.0,>=1.10.1->neupy) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow<1.14.0,>=1.10.1->neupy) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow<1.14.0,>=1.10.1->neupy) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow<1.14.0,>=1.10.1->neupy) (3.10.0.2)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->neupy) (1.5.2)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow, progressbar2, graphviz, neupy\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: progressbar2\n",
            "    Found existing installation: progressbar2 3.38.0\n",
            "    Uninstalling progressbar2-3.38.0:\n",
            "      Successfully uninstalled progressbar2-3.38.0\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.13.2 which is incompatible.\u001b[0m\n",
            "Successfully installed graphviz-0.5.1 keras-applications-1.0.8 mock-4.0.3 neupy-0.8.2 progressbar2-3.34.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Dataset"
      ],
      "metadata": {
        "id": "kRMMpjRN5XfD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k8R-LcZ7lXI",
        "outputId": "4ef347ca-ba48-45fe-f548-f6f63d1d7d3e"
      },
      "source": [
        "!wget 'https://drive.google.com/uc?export=download&id=1wCpUu5mCfVUHVTmMBo7sqTax5F7EDfrW' -O datasetalquran.xlsx "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-25 02:08:09--  https://drive.google.com/uc?export=download&id=1wCpUu5mCfVUHVTmMBo7sqTax5F7EDfrW\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.195.113, 74.125.195.138, 74.125.195.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.195.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/dtnib5sdm9n79lpfbjveht6o8mmv053f/1643076450000/16601388677960215569/*/1wCpUu5mCfVUHVTmMBo7sqTax5F7EDfrW?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-01-25 02:08:11--  https://doc-00-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/dtnib5sdm9n79lpfbjveht6o8mmv053f/1643076450000/16601388677960215569/*/1wCpUu5mCfVUHVTmMBo7sqTax5F7EDfrW?e=download\n",
            "Resolving doc-00-04-docs.googleusercontent.com (doc-00-04-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-00-04-docs.googleusercontent.com (doc-00-04-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1060790 (1.0M) [application/vnd.openxmlformats-officedocument.spreadsheetml.sheet]\n",
            "Saving to: ‘datasetalquran.xlsx’\n",
            "\n",
            "datasetalquran.xlsx 100%[===================>]   1.01M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-01-25 02:08:13 (97.8 MB/s) - ‘datasetalquran.xlsx’ saved [1060790/1060790]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-3YPpGHDnO-"
      },
      "source": [
        "df = pd.read_excel('datasetalquran.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v-jvwxtwUBe"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kelas={}\n",
        "#case folding\n",
        "df['Terjemahan']=df['Terjemahan'].apply(lambda x: x.lower())\n",
        "#punctual removal\n",
        "df['Terjemahan']=df['Terjemahan'].apply(lambda x: re.sub('[^a-zA-z\\s]','',x))\n",
        "#stemming\n",
        "ps=PorterStemmer()\n",
        "df['Terjemahan']=df['Terjemahan'].apply(lambda x: \" \".join([ps.stem(y) for y in x.split(\" \")]))      \n",
        "#stopword removal\n",
        "stop_word= set (stopwords.words(\"english\"))\n",
        "stop_word.remove('did') \n",
        "df['Terjemahan']=df['Terjemahan'].apply(lambda x: \" \".join([item for item in x.split(\" \") if item not in stop_word]))"
      ],
      "metadata": {
        "id": "EQCD-Bh0OIgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inisiasi\n",
        "k=0;\n",
        "tohammingloss=[]\n",
        "\n",
        "model = GaussianNB()\n",
        "model2 = MultinomialNB()\n",
        "model3 = ComplementNB()\n",
        "model4 = BernoulliNB()\n",
        "model5 = RandomForestClassifier(n_estimators=10)\n",
        "model6 = tree.DecisionTreeClassifier()\n",
        "model7 = GradientBoostingClassifier(n_estimators=10)\n",
        "model8 = VotingClassifier(estimators=[('mnb', model2), ('cnb', model2), ('bnb', model2)],voting='soft')\n",
        "model9 = VotingRegressor(estimators=[('mnb', model2), ('cnb', model2), ('bnb', model2)])"
      ],
      "metadata": {
        "id": "PXYWEoIbOfWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hm=[]\n",
        "kf=KFold(n_splits=5)#inisiasi kfold\n",
        "for train, test in kf.split(df):#jadi 5 train dan 5 test\n",
        "    allpredik=pd.DataFrame()#inislaisai\n",
        "    train_data = np.array(df)[train]#simpan index data\n",
        "    test_data = np.array(df)[test]#simpan index data\n",
        "    X_train=pd.Series(np.resize(train_data[:,[3]],(train_data[:,[0]].size,)), dtype='str')#ambil data ayat\n",
        "    X_test=pd.Series(np.resize(test_data[:,[3]],(test_data[:,[0]].size,)), dtype='str')#ambil data ayat\n",
        "    allytest=test_data[:,[4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]]#ambil data label \n",
        "    tokenizer = Tokenizer( split=' ')\n",
        "    tokenizer.fit_on_texts(X_train)#ambil tiap kata\n",
        "    X_train1 = tokenizer.texts_to_matrix(X_train, mode='tfidf') #tokenisasi dan TFIDF\n",
        "    X_test=tokenizer.texts_to_matrix(X_test, mode='tfidf') #tokenisasi dan TFIDF\n",
        "    for kelas in range(15):       \n",
        "      Y_train=train_data[:,[kelas+4]]\n",
        "      Y_test=test_data[:,[kelas+4]]\n",
        "      # X_train, Y_train = resample(X_train1, Y_train.ravel().astype(int))\n",
        "      # if skenariosampling==\"smote\":\n",
        "      X_train=X_train1\n",
        "      Y_train=Y_train.ravel().astype(int)\n",
        "      # print(X_train.shape,Y_train.shape)\n",
        "      # print(X_test.shape,Y_test.shape)\n",
        "      gnb = model.fit(X_train, Y_train)\n",
        "      # mnb = model2.fit(X_train, Y_train)\n",
        "      # cnb = model3.fit(X_train, Y_train)\n",
        "      # bnb = model4.fit(X_train, Y_train)\n",
        "      # lr = model5.fit(X_train, Y_train)\n",
        "      # svm = model6.fit(X_train, Y_train)\n",
        "      # rf = model7.fit(X_train, Y_train)\n",
        "      # vc = model8.fit(X_train, Y_train)\n",
        "      # vr = model9.fit(X_train, Y_train)\n",
        "      predik1 = gnb.predict(X_test)\n",
        "      # predik2 = mnb.predict(X_test)\n",
        "      # predik3 = cnb.predict(X_test)\n",
        "      # predik4 = bnb.predict(X_test)\n",
        "      # predik5 = lr.predict(X_test)\n",
        "      # predik6 = svm.predict(X_test)\n",
        "      # predik7 = rf.predict(X_test)\n",
        "      # predik8 = vc.predict(X_test)\n",
        "      # predik9 = vr.predict(X_test)\n",
        "      tablepred = []#simpan predik 1 kelas\n",
        "      for i in range (0,len(X_test)):\n",
        "        # if sum([predik2[i],predik3[i],predik4[i]]) == 3:\n",
        "        if sum([predik1[i]]) == 1:\n",
        "        # if sum([predik3[i],predik4[i]]) > 1:\n",
        "        # if sum([predik3[i],predik1[i],predik4[i]]) > 1:\n",
        "        # if sum([predik1[i],predik2[i],predik3[i],predik4[i]]) > 2:\n",
        "          tablepred.append(1)\n",
        "        else:\n",
        "          tablepred.append(0)\n",
        "      allpredik[str(kelas+1)]=tablepred\n",
        "    #predik jadi array\n",
        "    p=[]\n",
        "    for pr  in np.array(allpredik):\n",
        "      if sum(pr)==0:\n",
        "        p.append(np.append(pr,1).tolist())\n",
        "      else:\n",
        "        p.append(np.append(pr,0).tolist())\n",
        "    # hasil prediksi\n",
        "    predik=np.asarray(p,dtype=\"int\")\n",
        "    #  menghitung hamming loss\n",
        "    print(\"Hammming loss: %.4f\" % (hamming_loss(allytest.astype(dtype='int32'),predik)))\n",
        "    #  menghitung hamming loss\n",
        "    hm.append(hamming_loss(allytest.astype(dtype='int32'),predik))\n",
        "    print(hm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APRXpbyuQU2b",
        "outputId": "24102c52-506f-4c3d-aee4-6989ff983d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hammming loss: 0.2451\n",
            "[0.24514222756410256]\n",
            "Hammming loss: 0.2783\n",
            "[0.24514222756410256, 0.2783179631114675]\n",
            "Hammming loss: 0.3479\n",
            "[0.24514222756410256, 0.2783179631114675, 0.3479350441058541]\n",
            "Hammming loss: 0.3592\n",
            "[0.24514222756410256, 0.2783179631114675, 0.3479350441058541, 0.3592121090617482]\n",
            "Hammming loss: 0.3908\n",
            "[0.24514222756410256, 0.2783179631114675, 0.3479350441058541, 0.3592121090617482, 0.3907878909382518]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(hm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMkKH2SNBX-K",
        "outputId": "6313c363-a5b4-47bc-e047-e4e48609d504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.32427904695628484"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dntSGla6Hxp-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}